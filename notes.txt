for copy:

docker cp <path to file on local system> container_name:<path inside container>

docker cp container_name:<path inside container> <path on local system to copy to>

===========================================

naming/tagging images and containers:

to run a container with a specific name -
docker run -p 3000:80 -d --rm --name some-name 25dk34kd003k

to name/tag an image -
docker build ./Dockerfile -t name:tag

===========================================

dockerhub: (first make sure to login with docker login)

docker push imageName
docker pull imageName

to tag -
docker tag imageName:version myDockerRepoName/imageProjectName

once tagged, then we can push (real example) -
docker tag myNodeImage:10.0.1 transientcw/node-project
docker push transientcw/node-project

============================================

volumes:

A docker volume is a host file system drive/folder that is mounted ("made available/mapped") into
containers.

You can easily mount localhost:home/myVolume ------> /app/persistentStoreData, and now when the container writes data into its
/app/persistenStoreData directory, that data will persist even when the container is stopped and removed, since containers are epehmeral.
This is how we will eventually create an Oracle DB container which will map to the host file system volume.

Volumes are different from the COPY command, in that COPY will take a one-time snapshot of the files/folders you are copying, and will
write them into the image. It's a single serving command, whereas volumes are persistent. Also, changes in the volume mount on the host
system will reflect instantly into the containers file system mapped volume directory, and vice versa

When you add a volume to a container, that volume will persist even when you stop/remove the container and start a new one.

To add a volume to an image -
VOLUME ["/path-to-container-volume-dir"]

There are 2 types of external data storages
volumes (managed by docker), and bind mounts (managed by you)
Volumes can be anonymous (like the example a few lines up, its managed by docker and the volume is at some unknown location - run docker volume ls to find the random name), or
volumes can be named volumes.

If you use an anonymous volume, docker will create and manage it during the lifecycle of the container - once the container shuts down, the anonymous volume is deleted.

With named volumes, you are able to define a specific name for your volume, and when the container shuts down, docker does NOT auto delete the volume.

Named volumes are perfect for data that needs to persist, but that doesn't need to be edited directly (even though the volume is named and saved after container shutdown, we still
cant access the named volume that docker creates and manages and keeps after a container shutdown).

When you want to run a named volume, you do not add the VOLUME keyword to your Dockerfile like above, you actually configure it during docker container run....
docker run -v userDefinedVolumeName:/path/to/container/directory

...............

When you need to persist data, and you need to potentially edit it directly on the host file system .... BIND MOUNTS TO THE RESCUE!
A perfect case is when you are building or editing an application that you have running in a container. If you wanted to add some html, you
would have to rebuild the image on every single change of code.... With a bind mount, you can update realtime code changes.

With bind mounts, the developer has total control of the mappings of the persistent directories in the host and container fs.

Bind mounts are set up in the run command because they are specific to containers, not images -

docker run -v "/absolute/path/to/host/system/mount/folder:/path-to-container-folder-to-map"       ** (notice the paths are wrapped in quotes)

now keep in mind - in the example app from the tutorial, we are copying package.json and running an npm install. Without running npm install locally inside our host system, if we decide to
bind mount the ENTIRE local app directory, rather than just the local app/public directory (the directory where the html code lives), essentially our local file system will overwrite the entire contents of the containers app directory - which in turn will completely delete the node_modules directory, and will throw an error because the container cannot find express/node
to spin its app up.... in order to tell docker to IGNORE a specific directory when we bind mount the entire app dir, we need to add one more anonymous volume...
For bind mounting the entire app/ directory into the container, and adding the anonymous volume to ignore the node_modules dir, use:

docker run -d --rm -p 3000:80 --name feedback-app -v feedback:/app/feedback -v "/Home/some/local/path/app:/app" -v /app/node_modules node-feedback-img

The above command sets up 2 anonymous volumes and a bind mount (the first -v is an anonymous volume persisting the feedback dir to /app/feedback in the container, the second -v is a bind mount which is mounting our entire local /app folder, and the third is another anonymous volume that is keeping the image structure of the /node_modules dir inside the container which already has an npm install

If you just wanted to bind mount the application code in /local/app/public dir, you could use
docker run -d --rm -p 3000:80 --name feedback-app -v feedback:/app/feedback -v "/Home/some/local/path/app/public:/app/public" node-feedback-img

The first -v is an anonymous volume which persists the /app/feedback dir in the container to our local feedback dir, and the second -v is a bind mount that overwrites JUST the public dir which contains application code, thus keeping the structure of the node_modules dir inside the container intact

.......................

read-only volumes:

Used for when we do NOT want the container itself to be able to change any files/folders on the host system, but when the host system should be able to change the files and reflect those changes in the container. By default, docker volumes are read/write. This is very simple, after you docker run and define a volume, you simply append the volume definition with :ro

docker run -p 3000:80 --name some-container -v "/host/file/path:/app:ro" -v anonVolume:/app/volume:ro image-name

This simply enforces that docker cannot make changes to the volume and its subfolder

................

notes:

you can also use docker volume create TEST-VOLUME for manually creating docker volumes (if you don't create a volume manually, docker will create it for you with named volumes)

to use, docker run -v TEST-VOLUME:/app/feedback


summary:

anonymous volume: -v /app/container-folder (does not survive container removal)
named volume: -v myName:/app/container-folder (does survive container removal)
bind mount: -v "/host/system/path:/container/path" (does survive container removal)

==============================================================

.dockerignore

You can specifically omit stuff just like a .gitignore

for instance node_modules (in this case, it would be for the purpose of IF there was a node_modules somehow on localhost, we would ensure it does not get copied over and be stale)

=============================================================

Arguments and environment variables:

Arguments are available inside the Dockerfile, not accessible in CMD or other application code. They are set in the image build, i.e. docker build --build-Arguments
Environment variables, are available both in the Dockerfile and inside application code at runtime. You set them with --env with docker run, or in ENV inside Dockerfile

Environment variable example - for our node app - inside app.listen(80), we have hardcoded 80 in the app. If you want to pass an environment variable through docker, we can instead access app.listen(process.env.PORT) (where PORT is the actual environment variable name - now we can pass a variable called PORT with any value when we run the container, either in the Dockerfile with ENV or when running the container via --env VARIABLENAME=VALUE (PORT=80)      ----- keep in mind, env variables passed in docker run will override Dockerfile


==============================================================

Networking -

if you want to use container to host comms, you don't need a network, you must use the internal domain which is host.docker.internal instead
of localhost. So for talking to a mongo or any running server/database, etc, in our networking app, instead of mongodb://localhost:27107, you
will use mongodb://host.docker.internal:27107/swfavorites to hit our locally host running mongodb schema

Container to container comms -

In order to have container to container comms, one way would be to get our mongo container up and running. Once its running, by using docker
inspect, we can get its ip address. The problem with this is that we are hard coding the ip address of the running mongo container. The ip
address will change evertime you spin up a new container, so we need a more elegant dynamic solution. In this case, we need to create a docker
network. Whichever containers you need to be able to talk to each other, you need to first create a new docker network, then whichever containers
are involved, when you run, add --network <networkName>. Then , instead of using localhost, or host.docker.internal, now we don't need an ip address,
you can literally just use the name of the container (if you named it use the custom name or use the auto generated name).
so our mongoose connectiton should be mongodb://mongo-container-name:27107/swfavorites. Since both containers are on the same docker network,
ip addresses will be auto resolved.

Summary -
use host.docker.internal to contact the host machine from a container

use container name for container-to-container comms, docker will know how to resolve name to ip dns, as long as any containers communicating
are on the same docker network. If not using the same network, you can use specific container ip addresses (not elegant).

Lastly, when creating a docker network, you can also specify the driver to use docker network create --driver <driverName> mynetwork. Different
network adapters can be used for networking features. Consult the docs, although most of the time youll use bridge driver, which is default.

=======================================================

Multi Container Apps

REQUIREMENTS for demo:
Dockerize the back end, mid tier, and react front end
0. create network + apply to containers
1. database data should persist
2. limit database access with a specific username and password (think about args/env variables for security)
3. data in the mid tier must persist (not the goals, the goals are persisted in the database) - persist the log files for the node mid tier
4. mid tier should have live source code, use a bind mount as well as nodemon for realtime node server code updates
5. for the front end, we need to be able to have live updates for source code

SOLUTIONS for demo: SUPER IMPORTANT, WHEN USING DOCKER RUN, PAY ATTENTION TO WHAT DIRECTORY YOU ARE IN BECAUSE OF THE $PWD IN YOUR
BIND MOUNTS!

DATABASE -
0. create network docker network create multi-net
1. create a bind mount for the db container, pointing to the source code /persistence/mongo directory, using:
  docker run -d --rm --name mongo-container -v "$(pwd)/persistence/mongo:/data/db" (its important to run the docker run command from the app src
  directory, or the path to local bind mount will fail)
2. when running the mongo container, in order to create a root user for auth, use
  docker run -d --rm --name mongo-container -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=password mongo

3.
docker run -d --rm \
  --name mongo-cont \
  -v "$PWD/persistence/mongo:/data/db" \
  -e MONGO_INITDB_ROOT_USERNAME=chris \
  -e MONGO_INITDB_ROOT_PASSWORD=secret \
  --network multi-net mongo

MID TIER - (make sure to update the mongoose connection to use the username and password above!!!!!)
1. Since we are networking, in app.js instead of using host.docker.internal for the mongoose connection, use the db container name
instead. Also spin the container up on the same network as the mongo container so they can talk. We need to publish port 80 on 80
so that the ui browser code can hit the mid tier.
docker run -d --rm -p 80:80 --network multi-net --name midtier-cont midtier

2. Set up persistence, for the logs directory. A named volume will suffice
docker run -d --rm -p 80:80 --network multi-net --name midtier-cont -v logs:/app/logs midtier

3. Set up bind mount for real time code updates by mounting the entire local application code folder
docker run -d --rm -p 80:80 --network multi-net --name midtier-cont -v logs:/app/logs -v "$PWD:/app" \
  -v /app/node_modules midtier

UI/FRONT END -
1. Since it's a react app, it uses react-scripts to run, similar to the angular dev server. By default it runs port 3000,
so once you have created the Docker file (node base image, basically copy package.json, npm install, copy all other files, expose 3000,
 then run npm start),
you will have to run the container in interactive mode to keep the server up and running. Since we still want to be able
to use our browser for the front end, we want to publish the port. Since this code runs in the browser, it only knows localhost, so no need
to put it on the network - the browser would never be able to resolve container ip addresses so just expose the ports. (we will need to
make sure to publish port 80 on the midtier container so the browser can get to it)

docker run -it --rm --name ui-cont -p 3000:3000 ui

2. we need to persist the app for live code updates, so a bind mount for the entire front end directory is needed. Remember to omit node_modules
from being overwritten as well
docker run -it --rm --name ui-cont -p 3000:3000 -v "$PWD/frontend:/app" -v /app/node_modules ui

====================================

Docker Compose

Used for both building images, and starting/stopping containers with central config. It does NOT replace
Dockerfiles, or images/containers, it just makes working with those things easier

Clearly with the last multi container app, having to manage multiple containers and type long run commands
isn't elegant. With componse, via one or more configuration files, we can spin up lots of containers and
instead of using docker run, we use docker compose up. 
This command will check your config file and spin a bunch of containers up easily.

For the compose file (docker-compose.yml), the word 'service' is synonymous with 'container'.
--rm - by default docker will remove the stopped container
-d , when you run the docker compose, you can specify this in the command
-network , when using compose, any containers within the compose file will be automatically
  added to a dynamically generated network and will be available

docker-compose.yml:

version: "some-version" -> *1
services: -> *2
  container1: -> (the container name)
    build: ./some-dir -> *3 (optional)
    container_name: 'some-name' -> (friendly container name)
    image: 'mongo:latest' (or custom images 'transientcw/node:latest')
    volumes:
      - name -> *4
      - name
    environment: -> *5
      SOME_ENV_NAME: value -> *6
      - SOME_OTHER_NAME=value -> *7
    env_file: -> *8
      - ./env
    networks: -> *9
      - multi-net
    ports: -> *9.5
      - "80:80"
    stdin_open: true -> *9.6
    tty: true -> *9.7
    depends_on: -> *9.8
      - serviceName1
      - serviceName2

volumes: -> *10 (*NOTE ANONYMOUS VOLUMES AND BIND MOUNTS DO NOT NEED TO BE SPECIFIED, JUST NAMED)
  persistence: -> *11
networks: -> *12
  multi-net: -> *13  
        


*1. The version used to lock in the compose version and its features available
*2. The containers to define/manage
*3. Path to the dockerfile (if building) for this container/service
*4. here you define your volumes, it can be a list on the same indentation level for multiple volumes
*5. list of environment variables
*6 syntax 1 for env variables
*7 syntax 2 for env variables
*8 path to environment variable file
*9 option to specify a network to put this container on (assuming it has been created)
*9.6 for -it
*9.7 for -it
*9.5 specify port mappings
*9.8 specify a list of services that this service depends on (if dependency fails, container wont start)
*10 global definition of any named volumes, same indentation as services
*11 list of any named volumes used, in the above syntax. This is global, so if you reference this in more
  than one service definition, you can share the same volume
*12 global definition of any networks used, same indentation as services
*13 list of any networks used, in the above syntax

once you run docker-compose down, it will shut down all containers and remove them (the services in the file);
Keep in mind - if you want to remove all volumes too, you must specify the -v flag
docker-compose down -v (be careful, do you really wanna delete volumes?)

Also, if you want to force building on every single docker-compose up, you can use the --build command. By
default compose will simply first check to see if an image that is specified in the compose file exists
locally, and if it does, it won't rebuild.... this could bite if any code changes.... so to force:
docker-compose up --build

================================

"Utility" containers (not an official term)

Docker can be used for so much more than just running a web application... The concept of a utility container
is that a utility container will spin up and provide an environment, not neccessarily a useable application.

Heres a perfect example. Lets say something, via the command line or even in some external application somewhere
need to do some simple math. You could literally just run
docker run --name node-util -it -d node
What we have just done is created a container that is running in its own terminal mode, but that is
detached...... It's not doing much but is running the node runtime environment in itself.you could use it
to do some math

with the following command:
docker exec <containerName> <command>

lets try initializing an npm repo:
docker exec fervent_wu npm init

if we do that, it tries to initialize a repo, but is waiting for input, so :
docker exec -it fervent_wu npm init

It successfully initialized an npm repo, which is useless, but it served a purpose as nothing more than
a utility container instead of an application container

another syntax of what we just did is
docker run -it containerName npm init

Lets build an actual useable utility container, it will be used to spin up npm repos, without having
to actually install npm on the host machine!

FROM node:14-alpine

WORKDIR /util

Very simple, if we bind mount a local folder to the app working dir, we can spin the util container up
in interactive mode, run an npm init, and it will output the results of the npm init command on to our
local host!

docker run -d -it --rm --name utility-cont -v "$PWD/npm:/util" imageName

then execute
docker exec -it utility-cont npm init

Since we bind mounted to the npm directory, the above command will spit out a package.json after it finished the npm
init

ENTRYPOINT -
adding the ENTRYPOINT command in your docker file, is basically going to append itself to the front of whatever
command you pass to execute in your docker run command -
for example:

FROM node

WORKDIR /app

ENTRYPOINT ["npm"]

docker run -it -d containerName run start

with the ENTRYPOINT command, run start will be appended with the entrypoint variable, effectively making it:
docker run -it -d containerName npm run start

This is cool because it can help prevent screwing up all kinds of files - the ONLY variables you can pass to the docker
run command with the above entrypoint are npm commands, install, init, version, etc. If you try to run some other executable
command, such as rm -rf, its not going to work, because "npm rm -rf" isnt a valid npm command and will fail. So using
an entrypoint is a way of securing executable commands, especially when you have a bind mount that can fuck with the host
file system, not good!

To convert the above to docker compose:

version: "3.8"
services:
  npm-cont:
    container_name: "npm-cont"
    build: ./
    volumes:
      - ./npm:/util
    stdin_open: true
    tty: true

the issue is that in our dockerfile for this, we have an entrypoint ["npm"]..... So if we try to docker-compose up this,
it will basically just be trying to execute "npm", which is going to fail, youll just get an npm error saying, what are
you trying to do?

docker-compose has a "run" command, used for containers that arent yet up.

docker-compose run <serviceName> <command to be appended after the entrypoint>
so in our case, since I named our service/container npm-cont:

docker-compose run npm-cont init

then from there to install express, we can do
docker-compose run npm-cont install --save express

keep in mind, even though docker-compose up will automagically remove its containers when they shut down, with
docker-compose run, it will leave the container lingering after it shuts down. All we have to do is add --rm to
docker-compose run

docker-compose run --rm npm-cont install
