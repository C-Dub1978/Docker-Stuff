for copy:

docker cp <path to file on local system> container_name:<path inside container>

docker cp container_name:<path inside container> <path on local system to copy to>

===========================================

naming/tagging images and containers:

to run a container with a specific name -
docker run -p 3000:80 -d --rm --name some-name 25dk34kd003k

to name/tag an image -
docker build ./Dockerfile -t name:tag

===========================================

dockerhub: (first make sure to login with docker login)

docker push imageName
docker pull imageName

to tag -
docker tag imageName:version myDockerRepoName/imageProjectName

once tagged, then we can push (real example) -
docker tag myNodeImage:10.0.1 transientcw/node-project
docker push transientcw/node-project

============================================

volumes:

A docker volume is a host file system drive/folder that is mounted ("made available/mapped") into
containers.

You can easily mount localhost:home/myVolume ------> /app/persistentStoreData, and now when the container writes data into its
/app/persistenStoreData directory, that data will persist even when the container is stopped and removed, since containers are epehmeral.
This is how we will eventually create an Oracle DB container which will map to the host file system volume.

Volumes are different from the COPY command, in that COPY will take a one-time snapshot of the files/folders you are copying, and will
write them into the image. It's a single serving command, whereas volumes are persistent. Also, changes in the volume mount on the host
system will reflect instantly into the containers file system mapped volume directory, and vice versa

When you add a volume to a container, that volume will persist even when you stop/remove the container and start a new one.

To add a volume to an image -
VOLUME ["/path-to-container-volume-dir"]

There are 2 types of external data storages
volumes (managed by docker), and bind mounts (managed by you)
Volumes can be anonymous (like the example a few lines up, its managed by docker and the volume is at some unknown location - run docker volume ls to find the random name), or
volumes can be named volumes.

If you use an anonymous volume, docker will create and manage it during the lifecycle of the container - once the container shuts down, the anonymous volume is deleted.

With named volumes, you are able to define a specific name for your volume, and when the container shuts down, docker does NOT auto delete the volume.

Named volumes are perfect for data that needs to persist, but that doesn't need to be edited directly (even though the volume is named and saved after container shutdown, we still
cant access the named volume that docker creates and manages and keeps after a container shutdown).

When you want to run a named volume, you do not add the VOLUME keyword to your Dockerfile like above, you actually configure it during docker container run....
docker run -v userDefinedVolumeName:/path/to/container/directory

...............

When you need to persist data, and you need to potentially edit it directly on the host file system .... BIND MOUNTS TO THE RESCUE!
A perfect case is when you are building or editing an application that you have running in a container. If you wanted to add some html, you
would have to rebuild the image on every single change of code.... With a bind mount, you can update realtime code changes.

With bind mounts, the developer has total control of the mappings of the persistent directories in the host and container fs.

Bind mounts are set up in the run command because they are specific to containers, not images -

docker run -v "/absolute/path/to/host/system/mount/folder:/path-to-container-folder-to-map"       ** (notice the paths are wrapped in quotes)

now keep in mind - in the example app from the tutorial, we are copying package.json and running an npm install. Without running npm install locally inside our host system, if we decide to
bind mount the ENTIRE local app directory, rather than just the local app/public directory (the directory where the html code lives), essentially our local file system will overwrite the entire contents of the containers app directory - which in turn will completely delete the node_modules directory, and will throw an error because the container cannot find express/node
to spin its app up.... in order to tell docker to IGNORE a specific directory when we bind mount the entire app dir, we need to add one more anonymous volume...
For bind mounting the entire app/ directory into the container, and adding the anonymous volume to ignore the node_modules dir, use:

docker run -d --rm -p 3000:80 --name feedback-app -v feedback:/app/feedback -v "/Home/some/local/path/app:/app" -v /app/node_modules node-feedback-img

The above command sets up 2 anonymous volumes and a bind mount (the first -v is an anonymous volume persisting the feedback dir to /app/feedback in the container, the second -v is a bind mount which is mounting our entire local /app folder, and the third is another anonymous volume that is keeping the image structure of the /node_modules dir inside the container which already has an npm install

If you just wanted to bind mount the application code in /local/app/public dir, you could use
docker run -d --rm -p 3000:80 --name feedback-app -v feedback:/app/feedback -v "/Home/some/local/path/app/public:/app/public" node-feedback-img

The first -v is an anonymous volume which persists the /app/feedback dir in the container to our local feedback dir, and the second -v is a bind mount that overwrites JUST the public dir which contains application code, thus keeping the structure of the node_modules dir inside the container intact

.......................

read-only volumes:

Used for when we do NOT want the container itself to be able to change any files/folders on the host system, but when the host system should be able to change the files and reflect those changes in the container. By default, docker volumes are read/write. This is very simple, after you docker run and define a volume, you simply append the volume definition with :ro

docker run -p 3000:80 --name some-container -v "/host/file/path:/app:ro" -v anonVolume:/app/volume:ro image-name

This simply enforces that docker cannot make changes to the volume and its subfolder

................

notes:

you can also use docker volume create TEST-VOLUME for manually creating docker volumes (if you don't create a volume manually, docker will create it for you with named volumes)

to use, docker run -v TEST-VOLUME:/app/feedback


summary:

anonymous volume: -v /app/container-folder (does not survive container removal)
named volume: -v myName:/app/container-folder (does survive container removal)
bind mount: -v "/host/system/path:/container/path" (does survive container removal)

==============================================================

.dockerignore

You can specifically omit stuff just like a .gitignore

for instance node_modules (in this case, it would be for the purpose of IF there was a node_modules somehow on localhost, we would ensure it does not get copied over and be stale)

=============================================================

Arguments and environment variables:

Arguments are available inside the Dockerfile, not accessible in CMD or other application code. They are set in the image build, i.e. docker build --build-Arguments
Environment variables, are available both in the Dockerfile and inside application code at runtime. You set them with --env with docker run, or in ENV inside Dockerfile

Environment variable example - for our node app - inside app.listen(80), we have hardcoded 80 in the app. If you want to pass an environment variable through docker, we can instead access app.listen(process.env.PORT) (where PORT is the actual environment variable name - now we can pass a variable called PORT with any value when we run the container, either in the Dockerfile with ENV or when running the container via --env VARIABLENAME=VALUE (PORT=80)      ----- keep in mind, env variables passed in docker run will override Dockerfile


==============================================================

Networking -

if you want to use container to host comms, you don't need a network, you must use the internal domain which is host.docker.internal instead
of localhost. So for talking to a mongo or any running server/database, etc, in our networking app, instead of mongodb://localhost:27107, you
will use mongodb://host.docker.internal:27107/swfavorites to hit our locally host running mongodb schema

Container to container comms -

In order to have container to container comms, one way would be to get our mongo container up and running. Once its running, by using docker
inspect, we can get its ip address. The problem with this is that we are hard coding the ip address of the running mongo container. The ip
address will change evertime you spin up a new container, so we need a more elegant dynamic solution. In this case, we need to create a docker
network. Whichever containers you need to be able to talk to each other, you need to first create a new docker network, then whichever containers
are involved, when you run, add --network <networkName>. Then , instead of using localhost, or host.docker.internal, now we don't need an ip address,
you can literally just use the name of the container (if you named it use the custom name or use the auto generated name).
so our mongoose connectiton should be mongodb://mongo-container-name:27107/swfavorites. Since both containers are on the same docker network,
ip addresses will be auto resolved.

Summary -
use host.docker.internal to contact the host machine from a container

use container name for container-to-container comms, docker will know how to resolve name to ip dns, as long as any containers communicating
are on the same docker network. If not using the same network, you can use specific container ip addresses (not elegant).






